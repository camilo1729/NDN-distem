# NDN-distem

* NDN experiments over Distem

This is the procedure to follow in order to repeat the experiments in Grid'5000.

First reserve the nodes with destructive option, because we are going to format =/tmp= directory with =btrfs=:

#+BEGIN_SRC sh
 $ oarsub -t deploy -t destructive -l slash_18=1+nodes=5,walltime=5:00:00 "sleep 1d"
#+END_SRC

We need 4 nodes for deploying =fai1= and 5 nodes for deploying =fai2=.
If we use less machine we will have problems when starting the daemons because of
insufficient memory.
Deploying environment:

#+BEGIN_SRC sh
 $ kadeploy3 -f $OAR_NODEFILE -e jessie-x64-nfs -k
#+END_SRC

Executing debootstrap with btrfs options:

#+BEGIN_SRC sh
 $ distem-bootstrap -g --debian-version jessie --enable-admin-network --btrfs-format /dev/sda5
#+END_SRC

Sometimes, it is useful to add the parameter =--max-vifaces 1024= to able able to create more vnodes per physical machine.
In the coordinator node, we need to install the following gems:

#+BEGIN_SRC sh
$ gem install ruby-cute net-scp
#+END_SRC

Cloning the repository in the coordinator node:

#+BEGIN_SRC sh
$ git clone https://github.com/camilo1729/NDN-distem.git
#+END_SRC

Create the file =exp.state.yaml= with the names of all physical machines that are running Distem.
You can use the script =expe_gen.rb= to generate such a file:

#+BEGIN_SRC sh
ruby exp_gen.rb machinefile "10.144.0.0/12" exp-data.yaml
#+END_SRC

There are too types of topologies:

- =fai1=: dandelion 3-2-4
- =fai2=: dandelion 5-2-20
- =fai500=: dandelion 5-2-50
- =fai1500=: dandelion 5-2-150

For building an infrastructure type:

#+BEGIN_SRC sh
$ ruby ndn_topo.rb exp-data.yaml fai1.yaml
#+END_SRC

And then start the NDN daemons:

#+BEGIN_SRC sh
$ ruby ndn_start.rb fai1.yaml
#+END_SRC


For big topologies =fai2=, =fai500=, =fai1500=, you have to use other images compile with other value of =MAX_NDN_PACKET_SIZE=.
This images are available on Nancy site:

- =jessie-ndn-datamax8800.tar.gz=
- =jessie-ndn-datamax17600.tar.gz=
- =jessie-ndn-datamax35200.tar.gz=
- =jessie-ndn-datamax70400.tar.gz=

You have to modify the image on the file =ndn_topo.rb=.
After deploying the topoloy and starting the =nfd= daemons. You have to veryfie that all the routing tables on all the nodes
are filled completely. To do this you can use the following command:

#+BEGIN_SRC sh
for i in $(cat machinefile.txt) ; do ssh $i-adm "nfd-status | grep nodeAnnounce | wc -l"; done
#+END_SRC

All the number shown have to be bigger than zero and very close between them.

* MAX_NDN_PACKET_SIZE bug

We downloa the header to modify the variable =MAX_NDN_PACKET_SIZE= located on the source file =ndn-ccx/src/encoding/tlv.hpp=

#+BEGIN_SRC sh
$ wget  https://raw.githubusercontent.com/named-data/ndn-cxx/4b4f754fbd1b79097c012d181b903b80397273b4/src/encoding/tlv.hpp
#+END_SRC

We can reuse the aformentioned topologies  but instead on using =ndn_topo.rb=, we use =ndn_topo_datamax.rb=

#+BEGIN_SRC sh
$ ruby ndn_topo_datamax.rb exp.state.yaml fai1.yaml
#+END_SRC

* Routing and Cache experiments

We will create a simple topology a dandelion 3-1  to test the NDN cache and NDN routing.

#+BEGIN_SRC sh
$ ruby ndn_topo.rb exp.state.yaml small_topo.yml
#+END_SRC


** Procedure for testing ping

Modify the file

#+BEGIN_SRC sh

$ cd ndn-cxx

$ vim examples/producer.cpp

#+END_SRC

we compile then

#+BEGIN_SRC sh
  ./waf configure --with-examples
  ./waf
  build/examples/producer &
#+END_SRC

You have to put the configured announced for the node for this case it will be =/ndn/nodeAnnounce1x0/testApp=,
then in the other machine you do a ndnping:

#+BEGIN_SRC
ndnping /ndn/nodeAnnounce1x0/testApp

#+END_SRC

You should be able to ping the machine.
** Procedure for testing cache with file transmission

We can use the application =ndncatchunks=. So we push a file in one node with the following command:

#+BEGIN_SRC sh

# we create first a text file
# This is a 96 MB file
yes | tr \\n x | head -c 100000000 > test_file.txt

ndnputchunks -f 100000 /ndn/nodeAnnounce0x0x0/bigfile < test_file.txt

#+END_SRC


Then, in the other nodes we perform:

#+BEGIN_SRC sh

time ndncatchunks  -l 10 -d iterative -p 20 /ndn/nodeAnnounce0x0x0/bigfile > download
#+END_SRC
We have to use the parameter -p to increase the pipeline and optimize the download time.
Values more than 20 cause some problem for downloading the file.

Here, we meausered the time to download.
* Ping all experiment

We have to deploy first the NDN testbed topology, we are going to
use the script which setups an infrastructure with latency by default =10ms=:

#+BEGIN_SRC sh
$ ruby ndn_topo_with_latency.rb exp.state.yaml ndn_testbed_topo.yaml
#+END_SRC

After you need to initiazile all the NDN middleware:

#+BEGIN_SRC sh
$ ruby ndn_start.rb exp.state.yaml ndn_testbed_topo.yaml
#+END_SRC

and then execute the script of the experiment

#+BEGIN_SRC sh
$ ruby ping_all_test.rb
#+END_SRC


This will generate several directories with the results of the experiment.

* Cache experiment

Before deploying, we setup the number of CS entries using the file =nfd.conf= by
changing the value of =cs_max_packets= variable. For the experiments, we use 20 and 262144 as values.
We have to deploy the fai1 topology which is a dandeleon 3-2-4.
Similarly, we use the script which setups an infrastructure with latency by default =10ms=:


#+BEGIN_SRC sh
$ ruby ndn_topo_with_latency.rb exp.state.yaml fai1.yaml
#+END_SRC

You initialize the NDN middleware:

#+BEGIN_SRC sh
$ ruby ndn_start.rb exp.state.yaml fai1.yaml
#+END_SRC

Then, we execute the different experiments:

** Cache all


We execute it as follows:

#+BEGIN_SRC sh
for i in $(cat file_sizes); do ruby cache_test.rb $i; done
#+END_SRC

** Calle all leaves

We execute it as follows:

#+BEGIN_SRC sh
for i in $(cat file_sizes); do ruby cache_test_leaf.rb $i; done
#+END_SRC

** Cache seq
Similarly we change the value of the file size to test in the file =cache_seq.rb=.

#+BEGIN_SRC sh
$ ruby cache_seq.rb &> cache_results
#+END_SRC
